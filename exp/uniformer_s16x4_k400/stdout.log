[06/12 15:04:48][INFO] train_net.py: 408: Train with config:
[06/12 15:04:48][INFO] train_net.py: 409: CfgNode({'BN': CfgNode({'USE_PRECISE_STATS': False, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1}), 'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 1, 'EVAL_PERIOD': 5, 'CHECKPOINT_PERIOD': 1, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': ()}), 'AUG': CfgNode({'ENABLE': True, 'NUM_SAMPLE': 2, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 1, 'NUM_SPATIAL_CROPS': 1, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'TEST_BEST': False}), 'RESNET': CfgNode({'TRANS_FUNC': 'bottleneck_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 1.0, 'DEPTH_FACTOR': 1.0, 'BOTTLENECK_FACTOR': 1.0, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'uniformer', 'MODEL_NAME': 'Uniformer', 'NUM_CLASSES': 400, 'LOSS_FUNC': 'soft_cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'uniformer'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'USE_CHECKPOINT': False, 'CHECKPOINT_NUM': [0, 0, 0, 0]}), 'MVIT': CfgNode({'MODE': 'conv', 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'UNIFORMER': CfgNode({'EMBED_DIM': [64, 128, 320, 512], 'DEPTH': [3, 4, 8, 3], 'HEAD_DIM': 64, 'MLP_RATIO': 4, 'QKV_BIAS': True, 'QKV_SCALE': None, 'REPRESENTATION_SIZE': None, 'DROPOUT_RATE': 0, 'ATTENTION_DROPOUT_RATE': 0, 'DROP_DEPTH_RATE': 0.1, 'PRETRAIN_NAME': 'uniformer_small_in1k', 'SPLIT': False, 'STAGE_TYPE': [0, 0, 1, 1], 'STD': False, 'PRUNE_RATIO': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], 'TRADE_OFF': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': './data_list/k400', 'PATH_LABEL_SEPARATOR': ',', 'PATH_PREFIX': '/super_faster_home/dataset/kinetics400', 'LABEL_PATH_TEMPLATE': 'somesomev1_rgb_{}_split.txt', 'IMAGE_TEMPLATE': '{:05d}.jpg', 'NUM_FRAMES': 16, 'SAMPLING_RATE': 4, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [256, 320], 'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0], 'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333], 'USE_OFFSET_SAMPLING': True, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'TARGET_FPS': 30, 'DECODING_BACKEND': 'decord', 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False}), 'SOLVER': CfgNode({'BASE_LR': 0.0001, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 110, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 10.0, 'WARMUP_START_LR': 1e-06, 'OPTIMIZING_METHOD': 'adamw', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRADIENT': 20}), 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './exp/uniformer_s16x4_k400', 'RNG_SEED': 6666, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': True, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[06/12 15:04:48][INFO] uniformer.py: 288: Use checkpoint: False
[06/12 15:04:48][INFO] uniformer.py: 289: Checkpoint number: [0, 0, 0, 0]
[06/12 15:04:48][INFO] uniformer.py: 413: Inflate: patch_embed1.proj.weight, torch.Size([64, 3, 4, 4]) => torch.Size([64, 3, 3, 4, 4])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: patch_embed2.proj.weight, torch.Size([128, 64, 2, 2]) => torch.Size([128, 64, 1, 2, 2])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: patch_embed3.proj.weight, torch.Size([320, 128, 2, 2]) => torch.Size([320, 128, 1, 2, 2])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: patch_embed4.proj.weight, torch.Size([512, 320, 2, 2]) => torch.Size([512, 320, 1, 2, 2])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.0.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.0.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.0.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.0.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.1.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.1.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.1.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.1.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.2.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.2.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.2.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.2.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.0.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.0.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.0.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.0.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.1.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.1.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.1.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.1.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.2.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.2.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.2.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.2.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.3.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.3.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.3.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.3.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.0.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.1.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.2.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.3.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.4.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.5.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.6.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks3.7.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks4.0.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks4.1.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 413: Inflate: blocks4.2.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:04:49][INFO] uniformer.py: 411: Ignore: head.weight
[06/12 15:04:49][INFO] uniformer.py: 411: Ignore: head.bias
[06/12 15:04:49][INFO] build.py:  45: load pretrained model
[06/12 15:04:49][INFO] misc.py: 183: Model:
Uniformer(
  (patch_embed1): SpeicalPatchEmbed(
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(3, 64, kernel_size=(3, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0))
  )
  (patch_embed2): PatchEmbed(
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed3): PatchEmbed(
    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(128, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed4): PatchEmbed(
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(320, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (pos_drop): Dropout(p=0, inplace=False)
  (blocks1): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): Identity()
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.006)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.012)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.018)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.024)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.029)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.035)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.041)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.047)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.053)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.059)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.065)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.071)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (6): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.076)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (7): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks4): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.088)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.094)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pre_logits): Identity()
  (head): Linear(in_features=512, out_features=400, bias=True)
)
[06/12 15:04:49][INFO] misc.py: 184: Params: 21,400,400
[06/12 15:04:49][INFO] misc.py: 185: Mem: 0.0800790786743164 MB
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:04:51][INFO] misc.py: 186: Flops: 41.756772352 G
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::layer_norm encountered 26 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::batch_norm encountered 15 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:04:51][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:04:51][INFO] misc.py: 191: Activations: 228.17576 M
[06/12 15:04:51][INFO] misc.py: 196: nvidia-smi
[06/12 15:04:51][INFO] kinetics.py:  76: Constructing Kinetics train...
[06/12 15:13:18][INFO] train_net.py: 408: Train with config:
[06/12 15:13:18][INFO] train_net.py: 409: CfgNode({'BN': CfgNode({'USE_PRECISE_STATS': False, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1}), 'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 1, 'EVAL_PERIOD': 5, 'CHECKPOINT_PERIOD': 1, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': ()}), 'AUG': CfgNode({'ENABLE': True, 'NUM_SAMPLE': 2, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 1, 'NUM_SPATIAL_CROPS': 1, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'TEST_BEST': False}), 'RESNET': CfgNode({'TRANS_FUNC': 'bottleneck_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 1.0, 'DEPTH_FACTOR': 1.0, 'BOTTLENECK_FACTOR': 1.0, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'uniformer', 'MODEL_NAME': 'Uniformer', 'NUM_CLASSES': 400, 'LOSS_FUNC': 'soft_cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'uniformer'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'USE_CHECKPOINT': False, 'CHECKPOINT_NUM': [0, 0, 0, 0]}), 'MVIT': CfgNode({'MODE': 'conv', 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'UNIFORMER': CfgNode({'EMBED_DIM': [64, 128, 320, 512], 'DEPTH': [3, 4, 8, 3], 'HEAD_DIM': 64, 'MLP_RATIO': 4, 'QKV_BIAS': True, 'QKV_SCALE': None, 'REPRESENTATION_SIZE': None, 'DROPOUT_RATE': 0, 'ATTENTION_DROPOUT_RATE': 0, 'DROP_DEPTH_RATE': 0.1, 'PRETRAIN_NAME': 'uniformer_small_in1k', 'SPLIT': False, 'STAGE_TYPE': [0, 0, 1, 1], 'STD': False, 'PRUNE_RATIO': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], 'TRADE_OFF': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': './data_list/k400', 'PATH_LABEL_SEPARATOR': ',', 'PATH_PREFIX': '/super_faster_home/dataset/kinetics400', 'LABEL_PATH_TEMPLATE': 'somesomev1_rgb_{}_split.txt', 'IMAGE_TEMPLATE': '{:05d}.jpg', 'NUM_FRAMES': 16, 'SAMPLING_RATE': 4, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [256, 320], 'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0], 'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333], 'USE_OFFSET_SAMPLING': True, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'TARGET_FPS': 30, 'DECODING_BACKEND': 'decord', 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False}), 'SOLVER': CfgNode({'BASE_LR': 0.0001, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 110, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 10.0, 'WARMUP_START_LR': 1e-06, 'OPTIMIZING_METHOD': 'adamw', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRADIENT': 20}), 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './exp/uniformer_s16x4_k400', 'RNG_SEED': 6666, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': True, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[06/12 15:13:18][INFO] uniformer.py: 288: Use checkpoint: False
[06/12 15:13:18][INFO] uniformer.py: 289: Checkpoint number: [0, 0, 0, 0]
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: patch_embed1.proj.weight, torch.Size([64, 3, 4, 4]) => torch.Size([64, 3, 3, 4, 4])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: patch_embed2.proj.weight, torch.Size([128, 64, 2, 2]) => torch.Size([128, 64, 1, 2, 2])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: patch_embed3.proj.weight, torch.Size([320, 128, 2, 2]) => torch.Size([320, 128, 1, 2, 2])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: patch_embed4.proj.weight, torch.Size([512, 320, 2, 2]) => torch.Size([512, 320, 1, 2, 2])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.0.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.0.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.0.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.0.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.1.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.1.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.1.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.1.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.2.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.2.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.2.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.2.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.0.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.0.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.0.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.0.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.1.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.1.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.1.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.1.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.2.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.2.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.2.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.2.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.3.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.3.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.3.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.3.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.0.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.1.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.2.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.3.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.4.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.5.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.6.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks3.7.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks4.0.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks4.1.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 413: Inflate: blocks4.2.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:13:18][INFO] uniformer.py: 411: Ignore: head.weight
[06/12 15:13:18][INFO] uniformer.py: 411: Ignore: head.bias
[06/12 15:13:18][INFO] build.py:  45: load pretrained model
[06/12 15:13:18][INFO] misc.py: 183: Model:
Uniformer(
  (patch_embed1): SpeicalPatchEmbed(
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(3, 64, kernel_size=(3, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0))
  )
  (patch_embed2): PatchEmbed(
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed3): PatchEmbed(
    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(128, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed4): PatchEmbed(
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(320, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (pos_drop): Dropout(p=0, inplace=False)
  (blocks1): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): Identity()
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.006)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.012)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.018)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.024)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.029)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.035)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.041)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.047)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.053)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.059)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.065)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.071)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (6): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.076)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (7): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks4): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.088)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.094)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pre_logits): Identity()
  (head): Linear(in_features=512, out_features=400, bias=True)
)
[06/12 15:13:18][INFO] misc.py: 184: Params: 21,400,400
[06/12 15:13:18][INFO] misc.py: 185: Mem: 0.0800790786743164 MB
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:13:19][INFO] misc.py: 186: Flops: 41.756772352 G
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::layer_norm encountered 26 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::batch_norm encountered 15 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:13:19][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:13:19][INFO] misc.py: 191: Activations: 228.17576 M
[06/12 15:13:19][INFO] misc.py: 196: nvidia-smi
[06/12 15:13:19][INFO] kinetics.py:  76: Constructing Kinetics train...
[06/12 15:14:10][INFO] train_net.py: 408: Train with config:
[06/12 15:14:10][INFO] train_net.py: 409: CfgNode({'BN': CfgNode({'USE_PRECISE_STATS': False, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1}), 'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 1, 'EVAL_PERIOD': 5, 'CHECKPOINT_PERIOD': 1, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': ()}), 'AUG': CfgNode({'ENABLE': True, 'NUM_SAMPLE': 2, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 1, 'NUM_SPATIAL_CROPS': 1, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'TEST_BEST': False}), 'RESNET': CfgNode({'TRANS_FUNC': 'bottleneck_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 1.0, 'DEPTH_FACTOR': 1.0, 'BOTTLENECK_FACTOR': 1.0, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'uniformer', 'MODEL_NAME': 'Uniformer', 'NUM_CLASSES': 400, 'LOSS_FUNC': 'soft_cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'uniformer'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'USE_CHECKPOINT': False, 'CHECKPOINT_NUM': [0, 0, 0, 0]}), 'MVIT': CfgNode({'MODE': 'conv', 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'UNIFORMER': CfgNode({'EMBED_DIM': [64, 128, 320, 512], 'DEPTH': [3, 4, 8, 3], 'HEAD_DIM': 64, 'MLP_RATIO': 4, 'QKV_BIAS': True, 'QKV_SCALE': None, 'REPRESENTATION_SIZE': None, 'DROPOUT_RATE': 0, 'ATTENTION_DROPOUT_RATE': 0, 'DROP_DEPTH_RATE': 0.1, 'PRETRAIN_NAME': 'uniformer_small_in1k', 'SPLIT': False, 'STAGE_TYPE': [0, 0, 1, 1], 'STD': False, 'PRUNE_RATIO': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], 'TRADE_OFF': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': './data_list/k400', 'PATH_LABEL_SEPARATOR': ',', 'PATH_PREFIX': '/super_faster_home/dataset/kinetics400', 'LABEL_PATH_TEMPLATE': 'somesomev1_rgb_{}_split.txt', 'IMAGE_TEMPLATE': '{:05d}.jpg', 'NUM_FRAMES': 16, 'SAMPLING_RATE': 4, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [256, 320], 'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0], 'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333], 'USE_OFFSET_SAMPLING': True, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'TARGET_FPS': 30, 'DECODING_BACKEND': 'decord', 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False}), 'SOLVER': CfgNode({'BASE_LR': 0.0001, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 110, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 10.0, 'WARMUP_START_LR': 1e-06, 'OPTIMIZING_METHOD': 'adamw', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRADIENT': 20}), 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './exp/uniformer_s16x4_k400', 'RNG_SEED': 6666, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': True, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[06/12 15:14:10][INFO] uniformer.py: 288: Use checkpoint: False
[06/12 15:14:10][INFO] uniformer.py: 289: Checkpoint number: [0, 0, 0, 0]
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: patch_embed1.proj.weight, torch.Size([64, 3, 4, 4]) => torch.Size([64, 3, 3, 4, 4])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: patch_embed2.proj.weight, torch.Size([128, 64, 2, 2]) => torch.Size([128, 64, 1, 2, 2])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: patch_embed3.proj.weight, torch.Size([320, 128, 2, 2]) => torch.Size([320, 128, 1, 2, 2])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: patch_embed4.proj.weight, torch.Size([512, 320, 2, 2]) => torch.Size([512, 320, 1, 2, 2])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.0.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.0.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.0.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.0.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.1.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.1.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.1.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.1.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.2.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.2.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.2.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.2.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.0.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.0.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.0.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.0.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.1.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.1.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.1.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.1.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.2.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.2.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.2.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.2.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.3.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.3.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.3.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.3.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.0.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.1.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.2.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.3.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.4.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.5.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.6.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks3.7.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks4.0.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks4.1.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 413: Inflate: blocks4.2.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:14:10][INFO] uniformer.py: 411: Ignore: head.weight
[06/12 15:14:10][INFO] uniformer.py: 411: Ignore: head.bias
[06/12 15:14:10][INFO] build.py:  45: load pretrained model
[06/12 15:14:10][INFO] misc.py: 183: Model:
Uniformer(
  (patch_embed1): SpeicalPatchEmbed(
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(3, 64, kernel_size=(3, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0))
  )
  (patch_embed2): PatchEmbed(
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed3): PatchEmbed(
    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(128, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed4): PatchEmbed(
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(320, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (pos_drop): Dropout(p=0, inplace=False)
  (blocks1): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): Identity()
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.006)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.012)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.018)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.024)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.029)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.035)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.041)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.047)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.053)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.059)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.065)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.071)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (6): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.076)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (7): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks4): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.088)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.094)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pre_logits): Identity()
  (head): Linear(in_features=512, out_features=400, bias=True)
)
[06/12 15:14:10][INFO] misc.py: 184: Params: 21,400,400
[06/12 15:14:10][INFO] misc.py: 185: Mem: 0.0800790786743164 MB
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:14:11][INFO] misc.py: 186: Flops: 41.756772352 G
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::layer_norm encountered 26 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::batch_norm encountered 15 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:14:11][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:14:11][INFO] misc.py: 191: Activations: 228.17576 M
[06/12 15:14:11][INFO] misc.py: 196: nvidia-smi
[06/12 15:14:11][INFO] kinetics.py:  76: Constructing Kinetics train...
[06/12 15:15:36][INFO] train_net.py: 408: Train with config:
[06/12 15:15:36][INFO] train_net.py: 409: CfgNode({'BN': CfgNode({'USE_PRECISE_STATS': False, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1}), 'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 1, 'EVAL_PERIOD': 5, 'CHECKPOINT_PERIOD': 1, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': ()}), 'AUG': CfgNode({'ENABLE': True, 'NUM_SAMPLE': 2, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 1, 'NUM_SPATIAL_CROPS': 1, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'TEST_BEST': False}), 'RESNET': CfgNode({'TRANS_FUNC': 'bottleneck_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 1.0, 'DEPTH_FACTOR': 1.0, 'BOTTLENECK_FACTOR': 1.0, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'uniformer', 'MODEL_NAME': 'Uniformer', 'NUM_CLASSES': 400, 'LOSS_FUNC': 'soft_cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'uniformer'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'USE_CHECKPOINT': False, 'CHECKPOINT_NUM': [0, 0, 0, 0]}), 'MVIT': CfgNode({'MODE': 'conv', 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'UNIFORMER': CfgNode({'EMBED_DIM': [64, 128, 320, 512], 'DEPTH': [3, 4, 8, 3], 'HEAD_DIM': 64, 'MLP_RATIO': 4, 'QKV_BIAS': True, 'QKV_SCALE': None, 'REPRESENTATION_SIZE': None, 'DROPOUT_RATE': 0, 'ATTENTION_DROPOUT_RATE': 0, 'DROP_DEPTH_RATE': 0.1, 'PRETRAIN_NAME': 'uniformer_small_in1k', 'SPLIT': False, 'STAGE_TYPE': [0, 0, 1, 1], 'STD': False, 'PRUNE_RATIO': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], 'TRADE_OFF': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': './data_list/k400', 'PATH_LABEL_SEPARATOR': ',', 'PATH_PREFIX': '/super_faster_home/dataset/kinetics400', 'LABEL_PATH_TEMPLATE': 'somesomev1_rgb_{}_split.txt', 'IMAGE_TEMPLATE': '{:05d}.jpg', 'NUM_FRAMES': 16, 'SAMPLING_RATE': 4, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [256, 320], 'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0], 'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333], 'USE_OFFSET_SAMPLING': True, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'TARGET_FPS': 30, 'DECODING_BACKEND': 'decord', 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False}), 'SOLVER': CfgNode({'BASE_LR': 0.0001, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 110, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 10.0, 'WARMUP_START_LR': 1e-06, 'OPTIMIZING_METHOD': 'adamw', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRADIENT': 20}), 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './exp/uniformer_s16x4_k400', 'RNG_SEED': 6666, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': True, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[06/12 15:15:36][INFO] uniformer.py: 288: Use checkpoint: False
[06/12 15:15:36][INFO] uniformer.py: 289: Checkpoint number: [0, 0, 0, 0]
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: patch_embed1.proj.weight, torch.Size([64, 3, 4, 4]) => torch.Size([64, 3, 3, 4, 4])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: patch_embed2.proj.weight, torch.Size([128, 64, 2, 2]) => torch.Size([128, 64, 1, 2, 2])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: patch_embed3.proj.weight, torch.Size([320, 128, 2, 2]) => torch.Size([320, 128, 1, 2, 2])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: patch_embed4.proj.weight, torch.Size([512, 320, 2, 2]) => torch.Size([512, 320, 1, 2, 2])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.0.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.0.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.0.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.0.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.1.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.1.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.1.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.1.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.2.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.2.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.2.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.2.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.0.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.0.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.0.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.0.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.1.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.1.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.1.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.1.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.2.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.2.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.2.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.2.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.3.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.3.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.3.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.3.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.0.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.1.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.2.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.3.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.4.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.5.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.6.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks3.7.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks4.0.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks4.1.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 413: Inflate: blocks4.2.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:15:37][INFO] uniformer.py: 411: Ignore: head.weight
[06/12 15:15:37][INFO] uniformer.py: 411: Ignore: head.bias
[06/12 15:15:37][INFO] build.py:  45: load pretrained model
[06/12 15:15:37][INFO] misc.py: 183: Model:
Uniformer(
  (patch_embed1): SpeicalPatchEmbed(
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(3, 64, kernel_size=(3, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0))
  )
  (patch_embed2): PatchEmbed(
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed3): PatchEmbed(
    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(128, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed4): PatchEmbed(
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(320, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (pos_drop): Dropout(p=0, inplace=False)
  (blocks1): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): Identity()
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.006)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.012)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.018)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.024)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.029)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.035)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.041)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.047)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.053)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.059)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.065)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.071)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (6): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.076)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (7): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks4): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.088)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.094)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pre_logits): Identity()
  (head): Linear(in_features=512, out_features=400, bias=True)
)
[06/12 15:15:37][INFO] misc.py: 184: Params: 21,400,400
[06/12 15:15:37][INFO] misc.py: 185: Mem: 0.0800790786743164 MB
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:15:37][INFO] misc.py: 186: Flops: 41.756772352 G
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::layer_norm encountered 26 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::batch_norm encountered 15 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:15:37][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:15:37][INFO] misc.py: 191: Activations: 228.17576 M
[06/12 15:15:37][INFO] misc.py: 196: nvidia-smi
[06/12 15:15:37][INFO] kinetics.py:  76: Constructing Kinetics train...
[06/12 15:17:07][INFO] train_net.py: 408: Train with config:
[06/12 15:17:07][INFO] train_net.py: 409: CfgNode({'BN': CfgNode({'USE_PRECISE_STATS': False, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1}), 'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 1, 'EVAL_PERIOD': 5, 'CHECKPOINT_PERIOD': 1, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': ()}), 'AUG': CfgNode({'ENABLE': True, 'NUM_SAMPLE': 2, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 1, 'NUM_SPATIAL_CROPS': 1, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'TEST_BEST': False}), 'RESNET': CfgNode({'TRANS_FUNC': 'bottleneck_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 1.0, 'DEPTH_FACTOR': 1.0, 'BOTTLENECK_FACTOR': 1.0, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'uniformer', 'MODEL_NAME': 'Uniformer', 'NUM_CLASSES': 400, 'LOSS_FUNC': 'soft_cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'uniformer'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'USE_CHECKPOINT': False, 'CHECKPOINT_NUM': [0, 0, 0, 0]}), 'MVIT': CfgNode({'MODE': 'conv', 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'UNIFORMER': CfgNode({'EMBED_DIM': [64, 128, 320, 512], 'DEPTH': [3, 4, 8, 3], 'HEAD_DIM': 64, 'MLP_RATIO': 4, 'QKV_BIAS': True, 'QKV_SCALE': None, 'REPRESENTATION_SIZE': None, 'DROPOUT_RATE': 0, 'ATTENTION_DROPOUT_RATE': 0, 'DROP_DEPTH_RATE': 0.1, 'PRETRAIN_NAME': 'uniformer_small_in1k', 'SPLIT': False, 'STAGE_TYPE': [0, 0, 1, 1], 'STD': False, 'PRUNE_RATIO': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], 'TRADE_OFF': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': './data_list/k400', 'PATH_LABEL_SEPARATOR': ',', 'PATH_PREFIX': '/super_faster_home/dataset/kinetics400', 'LABEL_PATH_TEMPLATE': 'somesomev1_rgb_{}_split.txt', 'IMAGE_TEMPLATE': '{:05d}.jpg', 'NUM_FRAMES': 16, 'SAMPLING_RATE': 4, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [256, 320], 'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0], 'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333], 'USE_OFFSET_SAMPLING': True, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'TARGET_FPS': 30, 'DECODING_BACKEND': 'decord', 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False}), 'SOLVER': CfgNode({'BASE_LR': 0.0001, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 110, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 10.0, 'WARMUP_START_LR': 1e-06, 'OPTIMIZING_METHOD': 'adamw', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRADIENT': 20}), 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './exp/uniformer_s16x4_k400', 'RNG_SEED': 6666, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': True, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[06/12 15:17:07][INFO] uniformer.py: 288: Use checkpoint: False
[06/12 15:17:07][INFO] uniformer.py: 289: Checkpoint number: [0, 0, 0, 0]
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: patch_embed1.proj.weight, torch.Size([64, 3, 4, 4]) => torch.Size([64, 3, 3, 4, 4])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: patch_embed2.proj.weight, torch.Size([128, 64, 2, 2]) => torch.Size([128, 64, 1, 2, 2])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: patch_embed3.proj.weight, torch.Size([320, 128, 2, 2]) => torch.Size([320, 128, 1, 2, 2])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: patch_embed4.proj.weight, torch.Size([512, 320, 2, 2]) => torch.Size([512, 320, 1, 2, 2])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.0.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.0.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.0.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.0.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.1.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.1.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.1.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.1.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.2.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.2.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.2.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.2.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.0.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.0.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.0.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.0.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.1.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.1.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.1.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.1.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.2.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.2.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.2.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.2.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.3.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.3.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.3.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.3.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.0.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.1.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.2.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.3.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.4.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.5.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.6.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks3.7.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks4.0.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks4.1.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 413: Inflate: blocks4.2.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:17:07][INFO] uniformer.py: 411: Ignore: head.weight
[06/12 15:17:07][INFO] uniformer.py: 411: Ignore: head.bias
[06/12 15:17:07][INFO] build.py:  45: load pretrained model
[06/12 15:17:07][INFO] misc.py: 183: Model:
Uniformer(
  (patch_embed1): SpeicalPatchEmbed(
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(3, 64, kernel_size=(3, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0))
  )
  (patch_embed2): PatchEmbed(
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed3): PatchEmbed(
    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(128, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed4): PatchEmbed(
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(320, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (pos_drop): Dropout(p=0, inplace=False)
  (blocks1): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): Identity()
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.006)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.012)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.018)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.024)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.029)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.035)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.041)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.047)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.053)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.059)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.065)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.071)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (6): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.076)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (7): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks4): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.088)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.094)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pre_logits): Identity()
  (head): Linear(in_features=512, out_features=400, bias=True)
)
[06/12 15:17:07][INFO] misc.py: 184: Params: 21,400,400
[06/12 15:17:07][INFO] misc.py: 185: Mem: 0.0800790786743164 MB
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:17:08][INFO] misc.py: 186: Flops: 41.756772352 G
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::layer_norm encountered 26 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::batch_norm encountered 15 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:17:08][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:17:08][INFO] misc.py: 191: Activations: 228.17576 M
[06/12 15:17:08][INFO] misc.py: 196: nvidia-smi
[06/12 15:17:08][INFO] kinetics.py:  76: Constructing Kinetics train...
[06/12 15:17:33][INFO] train_net.py: 408: Train with config:
[06/12 15:17:33][INFO] train_net.py: 409: CfgNode({'BN': CfgNode({'USE_PRECISE_STATS': False, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1}), 'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 1, 'EVAL_PERIOD': 5, 'CHECKPOINT_PERIOD': 1, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': ()}), 'AUG': CfgNode({'ENABLE': True, 'NUM_SAMPLE': 2, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 1, 'NUM_SPATIAL_CROPS': 1, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'TEST_BEST': False}), 'RESNET': CfgNode({'TRANS_FUNC': 'bottleneck_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 1.0, 'DEPTH_FACTOR': 1.0, 'BOTTLENECK_FACTOR': 1.0, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'uniformer', 'MODEL_NAME': 'Uniformer', 'NUM_CLASSES': 400, 'LOSS_FUNC': 'soft_cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'uniformer'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'USE_CHECKPOINT': False, 'CHECKPOINT_NUM': [0, 0, 0, 0]}), 'MVIT': CfgNode({'MODE': 'conv', 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'UNIFORMER': CfgNode({'EMBED_DIM': [64, 128, 320, 512], 'DEPTH': [3, 4, 8, 3], 'HEAD_DIM': 64, 'MLP_RATIO': 4, 'QKV_BIAS': True, 'QKV_SCALE': None, 'REPRESENTATION_SIZE': None, 'DROPOUT_RATE': 0, 'ATTENTION_DROPOUT_RATE': 0, 'DROP_DEPTH_RATE': 0.1, 'PRETRAIN_NAME': 'uniformer_small_in1k', 'SPLIT': False, 'STAGE_TYPE': [0, 0, 1, 1], 'STD': False, 'PRUNE_RATIO': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], 'TRADE_OFF': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': './data_list/k400', 'PATH_LABEL_SEPARATOR': ',', 'PATH_PREFIX': '/super_faster_home/dataset/kinetics400', 'LABEL_PATH_TEMPLATE': 'somesomev1_rgb_{}_split.txt', 'IMAGE_TEMPLATE': '{:05d}.jpg', 'NUM_FRAMES': 16, 'SAMPLING_RATE': 4, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [256, 320], 'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0], 'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333], 'USE_OFFSET_SAMPLING': True, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'TARGET_FPS': 30, 'DECODING_BACKEND': 'decord', 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False}), 'SOLVER': CfgNode({'BASE_LR': 0.0001, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 110, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 10.0, 'WARMUP_START_LR': 1e-06, 'OPTIMIZING_METHOD': 'adamw', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRADIENT': 20}), 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './exp/uniformer_s16x4_k400', 'RNG_SEED': 6666, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': True, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[06/12 15:17:33][INFO] uniformer.py: 288: Use checkpoint: False
[06/12 15:17:33][INFO] uniformer.py: 289: Checkpoint number: [0, 0, 0, 0]
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: patch_embed1.proj.weight, torch.Size([64, 3, 4, 4]) => torch.Size([64, 3, 3, 4, 4])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: patch_embed2.proj.weight, torch.Size([128, 64, 2, 2]) => torch.Size([128, 64, 1, 2, 2])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: patch_embed3.proj.weight, torch.Size([320, 128, 2, 2]) => torch.Size([320, 128, 1, 2, 2])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: patch_embed4.proj.weight, torch.Size([512, 320, 2, 2]) => torch.Size([512, 320, 1, 2, 2])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.0.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.0.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.0.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.0.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.1.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.1.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.1.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.1.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.2.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.2.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.2.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.2.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.0.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.0.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.0.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.0.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.1.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.1.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.1.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.1.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.2.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.2.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.2.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.2.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.3.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.3.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.3.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.3.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.0.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.1.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.2.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.3.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.4.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.5.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.6.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks3.7.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks4.0.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks4.1.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 413: Inflate: blocks4.2.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:17:34][INFO] uniformer.py: 411: Ignore: head.weight
[06/12 15:17:34][INFO] uniformer.py: 411: Ignore: head.bias
[06/12 15:17:34][INFO] build.py:  45: load pretrained model
[06/12 15:17:34][INFO] misc.py: 183: Model:
Uniformer(
  (patch_embed1): SpeicalPatchEmbed(
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(3, 64, kernel_size=(3, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0))
  )
  (patch_embed2): PatchEmbed(
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed3): PatchEmbed(
    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(128, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed4): PatchEmbed(
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(320, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (pos_drop): Dropout(p=0, inplace=False)
  (blocks1): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): Identity()
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.006)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.012)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.018)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.024)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.029)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.035)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.041)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.047)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.053)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.059)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.065)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.071)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (6): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.076)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (7): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks4): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.088)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.094)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pre_logits): Identity()
  (head): Linear(in_features=512, out_features=400, bias=True)
)
[06/12 15:17:34][INFO] misc.py: 184: Params: 21,400,400
[06/12 15:17:34][INFO] misc.py: 185: Mem: 0.0800790786743164 MB
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:17:34][INFO] misc.py: 186: Flops: 41.756772352 G
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::layer_norm encountered 26 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::batch_norm encountered 15 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:17:34][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:17:34][INFO] misc.py: 191: Activations: 228.17576 M
[06/12 15:17:34][INFO] misc.py: 196: nvidia-smi
[06/12 15:17:34][INFO] kinetics.py:  76: Constructing Kinetics train...
[06/12 15:17:35][INFO] kinetics.py: 123: Constructing kinetics dataloader (size: 234583) from ./data_list/k400/train.csv
[06/12 15:17:35][INFO] kinetics.py:  76: Constructing Kinetics val...
[06/12 15:17:35][INFO] kinetics.py: 123: Constructing kinetics dataloader (size: 19760) from ./data_list/k400/val.csv
[06/12 15:17:35][INFO] tensorboard_vis.py:  54: To see logged results in Tensorboard, please launch using the command             `tensorboard  --port=<port-number> --logdir ./exp/uniformer_s16x4_k400/runs-kinetics`
[06/12 15:17:35][INFO] train_net.py: 451: Start epoch: 1
[06/12 15:17:57][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07387, "dt_data": 0.00216, "dt_net": 0.07171, "epoch": "1/110", "eta": "22 days, 1:30:13", "gpu_mem": "2.45G", "iter": "10/234583", "loss": 5.99908, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:17:57][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07604, "dt_data": 0.00236, "dt_net": 0.07369, "epoch": "1/110", "eta": "22 days, 17:03:45", "gpu_mem": "2.46G", "iter": "20/234583", "loss": 5.99603, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:17:58][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07304, "dt_data": 0.00168, "dt_net": 0.07136, "epoch": "1/110", "eta": "21 days, 19:30:06", "gpu_mem": "2.46G", "iter": "30/234583", "loss": 6.00247, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:17:59][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07722, "dt_data": 0.00228, "dt_net": 0.07494, "epoch": "1/110", "eta": "23 days, 1:31:56", "gpu_mem": "2.46G", "iter": "40/234583", "loss": 6.00361, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:00][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07771, "dt_data": 0.00190, "dt_net": 0.07580, "epoch": "1/110", "eta": "23 days, 4:59:27", "gpu_mem": "2.46G", "iter": "50/234583", "loss": 6.00342, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:00][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07354, "dt_data": 0.00178, "dt_net": 0.07176, "epoch": "1/110", "eta": "21 days, 23:07:57", "gpu_mem": "2.46G", "iter": "60/234583", "loss": 5.99338, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:01][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07441, "dt_data": 0.00179, "dt_net": 0.07262, "epoch": "1/110", "eta": "22 days, 5:20:12", "gpu_mem": "2.46G", "iter": "70/234583", "loss": 6.00167, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:02][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07362, "dt_data": 0.00176, "dt_net": 0.07186, "epoch": "1/110", "eta": "21 days, 23:40:52", "gpu_mem": "2.46G", "iter": "80/234583", "loss": 6.00007, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:03][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07493, "dt_data": 0.00187, "dt_net": 0.07307, "epoch": "1/110", "eta": "22 days, 9:06:03", "gpu_mem": "2.46G", "iter": "90/234583", "loss": 5.99489, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:03][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.08026, "dt_data": 0.00231, "dt_net": 0.07795, "epoch": "1/110", "eta": "23 days, 23:19:16", "gpu_mem": "2.46G", "iter": "100/234583", "loss": 5.99911, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:04][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07441, "dt_data": 0.00169, "dt_net": 0.07272, "epoch": "1/110", "eta": "22 days, 5:19:15", "gpu_mem": "2.46G", "iter": "110/234583", "loss": 5.99997, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:05][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07508, "dt_data": 0.00184, "dt_net": 0.07323, "epoch": "1/110", "eta": "22 days, 10:08:39", "gpu_mem": "2.46G", "iter": "120/234583", "loss": 5.99361, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:06][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07428, "dt_data": 0.00182, "dt_net": 0.07246, "epoch": "1/110", "eta": "22 days, 4:25:17", "gpu_mem": "2.46G", "iter": "130/234583", "loss": 5.99934, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:06][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07459, "dt_data": 0.00184, "dt_net": 0.07275, "epoch": "1/110", "eta": "22 days, 6:38:26", "gpu_mem": "2.46G", "iter": "140/234583", "loss": 5.99775, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:07][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07528, "dt_data": 0.00173, "dt_net": 0.07356, "epoch": "1/110", "eta": "22 days, 11:37:12", "gpu_mem": "2.46G", "iter": "150/234583", "loss": 5.99531, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:08][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07668, "dt_data": 0.00258, "dt_net": 0.07410, "epoch": "1/110", "eta": "22 days, 21:39:09", "gpu_mem": "2.46G", "iter": "160/234583", "loss": 5.99638, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:09][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07491, "dt_data": 0.00176, "dt_net": 0.07314, "epoch": "1/110", "eta": "22 days, 8:54:26", "gpu_mem": "2.46G", "iter": "170/234583", "loss": 5.99696, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:09][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.08125, "dt_data": 0.00227, "dt_net": 0.07898, "epoch": "1/110", "eta": "24 days, 6:23:45", "gpu_mem": "2.46G", "iter": "180/234583", "loss": 5.99741, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:10][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07769, "dt_data": 0.00270, "dt_net": 0.07499, "epoch": "1/110", "eta": "23 days, 4:51:03", "gpu_mem": "2.46G", "iter": "190/234583", "loss": 5.99704, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:11][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07926, "dt_data": 0.00168, "dt_net": 0.07758, "epoch": "1/110", "eta": "23 days, 16:08:18", "gpu_mem": "2.46G", "iter": "200/234583", "loss": 5.99558, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:12][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07549, "dt_data": 0.00223, "dt_net": 0.07326, "epoch": "1/110", "eta": "22 days, 13:04:59", "gpu_mem": "2.46G", "iter": "210/234583", "loss": 6.00613, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:13][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07635, "dt_data": 0.00248, "dt_net": 0.07387, "epoch": "1/110", "eta": "22 days, 19:16:35", "gpu_mem": "2.46G", "iter": "220/234583", "loss": 6.00316, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:13][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07827, "dt_data": 0.00205, "dt_net": 0.07623, "epoch": "1/110", "eta": "23 days, 9:02:24", "gpu_mem": "2.46G", "iter": "230/234583", "loss": 5.99520, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:14][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07392, "dt_data": 0.00205, "dt_net": 0.07186, "epoch": "1/110", "eta": "22 days, 1:49:03", "gpu_mem": "2.46G", "iter": "240/234583", "loss": 6.00372, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:15][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07600, "dt_data": 0.00189, "dt_net": 0.07411, "epoch": "1/110", "eta": "22 days, 16:44:26", "gpu_mem": "2.46G", "iter": "250/234583", "loss": 5.99902, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:16][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07811, "dt_data": 0.00228, "dt_net": 0.07582, "epoch": "1/110", "eta": "23 days, 7:52:04", "gpu_mem": "2.46G", "iter": "260/234583", "loss": 5.99888, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:16][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07716, "dt_data": 0.00217, "dt_net": 0.07499, "epoch": "1/110", "eta": "23 days, 1:03:30", "gpu_mem": "2.46G", "iter": "270/234583", "loss": 5.99586, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:17][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07543, "dt_data": 0.00197, "dt_net": 0.07346, "epoch": "1/110", "eta": "22 days, 12:39:12", "gpu_mem": "2.46G", "iter": "280/234583", "loss": 5.99564, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:18][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07470, "dt_data": 0.00199, "dt_net": 0.07272, "epoch": "1/110", "eta": "22 days, 7:26:47", "gpu_mem": "2.46G", "iter": "290/234583", "loss": 5.99502, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:19][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07606, "dt_data": 0.00180, "dt_net": 0.07426, "epoch": "1/110", "eta": "22 days, 17:10:27", "gpu_mem": "2.46G", "iter": "300/234583", "loss": 6.00024, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:19][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07929, "dt_data": 0.00168, "dt_net": 0.07761, "epoch": "1/110", "eta": "23 days, 16:19:04", "gpu_mem": "2.46G", "iter": "310/234583", "loss": 5.99565, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:20][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07504, "dt_data": 0.00238, "dt_net": 0.07266, "epoch": "1/110", "eta": "22 days, 9:50:40", "gpu_mem": "2.46G", "iter": "320/234583", "loss": 5.99407, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:21][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07602, "dt_data": 0.00191, "dt_net": 0.07411, "epoch": "1/110", "eta": "22 days, 16:54:37", "gpu_mem": "2.46G", "iter": "330/234583", "loss": 5.99752, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:22][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07461, "dt_data": 0.00197, "dt_net": 0.07264, "epoch": "1/110", "eta": "22 days, 6:46:20", "gpu_mem": "2.46G", "iter": "340/234583", "loss": 5.99427, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:22][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07616, "dt_data": 0.00173, "dt_net": 0.07443, "epoch": "1/110", "eta": "22 days, 17:54:24", "gpu_mem": "2.46G", "iter": "350/234583", "loss": 5.99314, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:23][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07474, "dt_data": 0.00167, "dt_net": 0.07307, "epoch": "1/110", "eta": "22 days, 7:40:58", "gpu_mem": "2.46G", "iter": "360/234583", "loss": 5.99412, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:24][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07425, "dt_data": 0.00169, "dt_net": 0.07257, "epoch": "1/110", "eta": "22 days, 4:12:33", "gpu_mem": "2.46G", "iter": "370/234583", "loss": 6.00567, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:25][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07549, "dt_data": 0.00167, "dt_net": 0.07382, "epoch": "1/110", "eta": "22 days, 13:05:59", "gpu_mem": "2.46G", "iter": "380/234583", "loss": 6.00254, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:26][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07486, "dt_data": 0.00164, "dt_net": 0.07322, "epoch": "1/110", "eta": "22 days, 8:33:19", "gpu_mem": "2.46G", "iter": "390/234583", "loss": 5.99373, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:26][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07657, "dt_data": 0.00185, "dt_net": 0.07472, "epoch": "1/110", "eta": "22 days, 20:48:55", "gpu_mem": "2.46G", "iter": "400/234583", "loss": 5.99149, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:27][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07583, "dt_data": 0.00169, "dt_net": 0.07414, "epoch": "1/110", "eta": "22 days, 15:31:29", "gpu_mem": "2.46G", "iter": "410/234583", "loss": 5.99640, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:28][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07854, "dt_data": 0.00233, "dt_net": 0.07622, "epoch": "1/110", "eta": "23 days, 10:57:45", "gpu_mem": "2.46G", "iter": "420/234583", "loss": 5.99813, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:29][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07457, "dt_data": 0.00189, "dt_net": 0.07269, "epoch": "1/110", "eta": "22 days, 6:31:13", "gpu_mem": "2.46G", "iter": "430/234583", "loss": 5.98832, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:29][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07664, "dt_data": 0.00179, "dt_net": 0.07485, "epoch": "1/110", "eta": "22 days, 21:21:18", "gpu_mem": "2.46G", "iter": "440/234583", "loss": 5.99658, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:30][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07821, "dt_data": 0.00198, "dt_net": 0.07623, "epoch": "1/110", "eta": "23 days, 8:35:06", "gpu_mem": "2.46G", "iter": "450/234583", "loss": 5.99757, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:31][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07550, "dt_data": 0.00216, "dt_net": 0.07334, "epoch": "1/110", "eta": "22 days, 13:09:24", "gpu_mem": "2.46G", "iter": "460/234583", "loss": 5.99996, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:32][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07620, "dt_data": 0.00183, "dt_net": 0.07437, "epoch": "1/110", "eta": "22 days, 18:10:44", "gpu_mem": "2.46G", "iter": "470/234583", "loss": 5.99508, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:32][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07647, "dt_data": 0.00250, "dt_net": 0.07397, "epoch": "1/110", "eta": "22 days, 20:06:42", "gpu_mem": "2.46G", "iter": "480/234583", "loss": 5.99910, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:33][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07525, "dt_data": 0.00187, "dt_net": 0.07338, "epoch": "1/110", "eta": "22 days, 11:23:33", "gpu_mem": "2.46G", "iter": "490/234583", "loss": 5.99914, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:34][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07841, "dt_data": 0.00176, "dt_net": 0.07665, "epoch": "1/110", "eta": "23 days, 10:01:05", "gpu_mem": "2.46G", "iter": "500/234583", "loss": 5.99843, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:35][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07827, "dt_data": 0.00188, "dt_net": 0.07639, "epoch": "1/110", "eta": "23 days, 9:01:27", "gpu_mem": "2.46G", "iter": "510/234583", "loss": 5.99958, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:35][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07613, "dt_data": 0.00182, "dt_net": 0.07431, "epoch": "1/110", "eta": "22 days, 17:39:44", "gpu_mem": "2.46G", "iter": "520/234583", "loss": 6.00271, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:36][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07491, "dt_data": 0.00173, "dt_net": 0.07318, "epoch": "1/110", "eta": "22 days, 8:57:38", "gpu_mem": "2.46G", "iter": "530/234583", "loss": 5.98888, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:37][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07671, "dt_data": 0.00181, "dt_net": 0.07490, "epoch": "1/110", "eta": "22 days, 21:50:07", "gpu_mem": "2.46G", "iter": "540/234583", "loss": 5.99756, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:38][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07755, "dt_data": 0.00178, "dt_net": 0.07577, "epoch": "1/110", "eta": "23 days, 3:51:53", "gpu_mem": "2.46G", "iter": "550/234583", "loss": 5.99812, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:39][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07484, "dt_data": 0.00171, "dt_net": 0.07313, "epoch": "1/110", "eta": "22 days, 8:25:43", "gpu_mem": "2.46G", "iter": "560/234583", "loss": 5.99660, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:39][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07497, "dt_data": 0.00169, "dt_net": 0.07328, "epoch": "1/110", "eta": "22 days, 9:21:57", "gpu_mem": "2.46G", "iter": "570/234583", "loss": 5.99275, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:40][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07528, "dt_data": 0.00179, "dt_net": 0.07349, "epoch": "1/110", "eta": "22 days, 11:34:05", "gpu_mem": "2.46G", "iter": "580/234583", "loss": 6.00047, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:41][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07745, "dt_data": 0.00261, "dt_net": 0.07484, "epoch": "1/110", "eta": "23 days, 3:07:52", "gpu_mem": "2.46G", "iter": "590/234583", "loss": 6.00030, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:42][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07692, "dt_data": 0.00224, "dt_net": 0.07467, "epoch": "1/110", "eta": "22 days, 23:19:46", "gpu_mem": "2.46G", "iter": "600/234583", "loss": 5.99445, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:42][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07495, "dt_data": 0.00195, "dt_net": 0.07299, "epoch": "1/110", "eta": "22 days, 9:11:11", "gpu_mem": "2.46G", "iter": "610/234583", "loss": 5.99845, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:43][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07448, "dt_data": 0.00176, "dt_net": 0.07272, "epoch": "1/110", "eta": "22 days, 5:48:52", "gpu_mem": "2.46G", "iter": "620/234583", "loss": 5.99454, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:44][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07662, "dt_data": 0.00180, "dt_net": 0.07482, "epoch": "1/110", "eta": "22 days, 21:11:11", "gpu_mem": "2.46G", "iter": "630/234583", "loss": 5.99364, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:45][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07559, "dt_data": 0.00178, "dt_net": 0.07381, "epoch": "1/110", "eta": "22 days, 13:48:16", "gpu_mem": "2.46G", "iter": "640/234583", "loss": 5.99423, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:45][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07583, "dt_data": 0.00205, "dt_net": 0.07378, "epoch": "1/110", "eta": "22 days, 15:30:12", "gpu_mem": "2.46G", "iter": "650/234583", "loss": 5.99842, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:46][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07506, "dt_data": 0.00179, "dt_net": 0.07327, "epoch": "1/110", "eta": "22 days, 9:58:06", "gpu_mem": "2.46G", "iter": "660/234583", "loss": 5.99306, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:47][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07746, "dt_data": 0.00175, "dt_net": 0.07571, "epoch": "1/110", "eta": "23 days, 3:12:53", "gpu_mem": "2.46G", "iter": "670/234583", "loss": 5.99862, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:48][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07529, "dt_data": 0.00203, "dt_net": 0.07326, "epoch": "1/110", "eta": "22 days, 11:39:28", "gpu_mem": "2.46G", "iter": "680/234583", "loss": 5.98915, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:48][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07651, "dt_data": 0.00237, "dt_net": 0.07414, "epoch": "1/110", "eta": "22 days, 20:23:31", "gpu_mem": "2.46G", "iter": "690/234583", "loss": 5.99382, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:49][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07575, "dt_data": 0.00222, "dt_net": 0.07353, "epoch": "1/110", "eta": "22 days, 14:56:16", "gpu_mem": "2.46G", "iter": "700/234583", "loss": 5.99580, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:50][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07362, "dt_data": 0.00167, "dt_net": 0.07195, "epoch": "1/110", "eta": "21 days, 23:42:49", "gpu_mem": "2.46G", "iter": "710/234583", "loss": 5.99602, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:51][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07569, "dt_data": 0.00177, "dt_net": 0.07393, "epoch": "1/110", "eta": "22 days, 14:32:33", "gpu_mem": "2.46G", "iter": "720/234583", "loss": 5.99242, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:51][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07580, "dt_data": 0.00176, "dt_net": 0.07405, "epoch": "1/110", "eta": "22 days, 15:20:14", "gpu_mem": "2.46G", "iter": "730/234583", "loss": 5.99297, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:52][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07757, "dt_data": 0.00232, "dt_net": 0.07525, "epoch": "1/110", "eta": "23 days, 3:58:44", "gpu_mem": "2.46G", "iter": "740/234583", "loss": 5.99866, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:53][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07709, "dt_data": 0.00171, "dt_net": 0.07538, "epoch": "1/110", "eta": "23 days, 0:32:07", "gpu_mem": "2.46G", "iter": "750/234583", "loss": 5.99393, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:54][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07815, "dt_data": 0.00179, "dt_net": 0.07636, "epoch": "1/110", "eta": "23 days, 8:09:49", "gpu_mem": "2.46G", "iter": "760/234583", "loss": 5.99256, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:55][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07697, "dt_data": 0.00233, "dt_net": 0.07465, "epoch": "1/110", "eta": "22 days, 23:43:20", "gpu_mem": "2.46G", "iter": "770/234583", "loss": 5.99703, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:55][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07800, "dt_data": 0.00219, "dt_net": 0.07580, "epoch": "1/110", "eta": "23 days, 7:02:16", "gpu_mem": "2.46G", "iter": "780/234583", "loss": 5.98997, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:56][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07695, "dt_data": 0.00186, "dt_net": 0.07509, "epoch": "1/110", "eta": "22 days, 23:32:09", "gpu_mem": "2.46G", "iter": "790/234583", "loss": 5.99698, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:57][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07517, "dt_data": 0.00207, "dt_net": 0.07310, "epoch": "1/110", "eta": "22 days, 10:48:54", "gpu_mem": "2.46G", "iter": "800/234583", "loss": 5.99493, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:58][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07601, "dt_data": 0.00175, "dt_net": 0.07426, "epoch": "1/110", "eta": "22 days, 16:47:30", "gpu_mem": "2.46G", "iter": "810/234583", "loss": 5.99339, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:58][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07556, "dt_data": 0.00187, "dt_net": 0.07369, "epoch": "1/110", "eta": "22 days, 13:35:40", "gpu_mem": "2.46G", "iter": "820/234583", "loss": 5.99441, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:18:59][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07585, "dt_data": 0.00167, "dt_net": 0.07418, "epoch": "1/110", "eta": "22 days, 15:38:02", "gpu_mem": "2.46G", "iter": "830/234583", "loss": 5.99693, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:00][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07574, "dt_data": 0.00207, "dt_net": 0.07367, "epoch": "1/110", "eta": "22 days, 14:53:49", "gpu_mem": "2.46G", "iter": "840/234583", "loss": 5.99514, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:01][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07878, "dt_data": 0.00169, "dt_net": 0.07709, "epoch": "1/110", "eta": "23 days, 12:38:38", "gpu_mem": "2.46G", "iter": "850/234583", "loss": 5.99233, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:01][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07480, "dt_data": 0.00178, "dt_net": 0.07302, "epoch": "1/110", "eta": "22 days, 8:07:59", "gpu_mem": "2.46G", "iter": "860/234583", "loss": 5.99863, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:02][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07488, "dt_data": 0.00167, "dt_net": 0.07321, "epoch": "1/110", "eta": "22 days, 8:43:37", "gpu_mem": "2.46G", "iter": "870/234583", "loss": 5.99589, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:03][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07563, "dt_data": 0.00194, "dt_net": 0.07369, "epoch": "1/110", "eta": "22 days, 14:03:31", "gpu_mem": "2.46G", "iter": "880/234583", "loss": 5.99800, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:04][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07598, "dt_data": 0.00170, "dt_net": 0.07428, "epoch": "1/110", "eta": "22 days, 16:37:22", "gpu_mem": "2.46G", "iter": "890/234583", "loss": 5.98980, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:04][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07750, "dt_data": 0.00199, "dt_net": 0.07551, "epoch": "1/110", "eta": "23 days, 3:29:12", "gpu_mem": "2.46G", "iter": "900/234583", "loss": 5.99124, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:05][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07898, "dt_data": 0.00201, "dt_net": 0.07696, "epoch": "1/110", "eta": "23 days, 14:05:27", "gpu_mem": "2.46G", "iter": "910/234583", "loss": 5.99419, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:06][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07462, "dt_data": 0.00171, "dt_net": 0.07291, "epoch": "1/110", "eta": "22 days, 6:50:20", "gpu_mem": "2.46G", "iter": "920/234583", "loss": 5.99903, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:07][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07412, "dt_data": 0.00188, "dt_net": 0.07224, "epoch": "1/110", "eta": "22 days, 3:16:41", "gpu_mem": "2.46G", "iter": "930/234583", "loss": 5.99784, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:08][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07608, "dt_data": 0.00237, "dt_net": 0.07372, "epoch": "1/110", "eta": "22 days, 17:20:22", "gpu_mem": "2.46G", "iter": "940/234583", "loss": 5.99325, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:08][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07660, "dt_data": 0.00192, "dt_net": 0.07468, "epoch": "1/110", "eta": "22 days, 21:03:35", "gpu_mem": "2.46G", "iter": "950/234583", "loss": 5.99597, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:09][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07471, "dt_data": 0.00185, "dt_net": 0.07286, "epoch": "1/110", "eta": "22 days, 7:30:42", "gpu_mem": "2.46G", "iter": "960/234583", "loss": 5.99595, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:10][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07679, "dt_data": 0.00210, "dt_net": 0.07469, "epoch": "1/110", "eta": "22 days, 22:23:04", "gpu_mem": "2.46G", "iter": "970/234583", "loss": 5.99688, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:11][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07544, "dt_data": 0.00191, "dt_net": 0.07353, "epoch": "1/110", "eta": "22 days, 12:44:40", "gpu_mem": "2.46G", "iter": "980/234583", "loss": 5.99415, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:11][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07859, "dt_data": 0.00203, "dt_net": 0.07656, "epoch": "1/110", "eta": "23 days, 11:15:49", "gpu_mem": "2.46G", "iter": "990/234583", "loss": 5.99744, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:12][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07659, "dt_data": 0.00174, "dt_net": 0.07484, "epoch": "1/110", "eta": "22 days, 20:56:50", "gpu_mem": "2.46G", "iter": "1000/234583", "loss": 5.99683, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:13][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07565, "dt_data": 0.00173, "dt_net": 0.07393, "epoch": "1/110", "eta": "22 days, 14:14:34", "gpu_mem": "2.46G", "iter": "1010/234583", "loss": 5.99551, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:14][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07607, "dt_data": 0.00177, "dt_net": 0.07430, "epoch": "1/110", "eta": "22 days, 17:14:24", "gpu_mem": "2.46G", "iter": "1020/234583", "loss": 5.99384, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:14][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07486, "dt_data": 0.00169, "dt_net": 0.07317, "epoch": "1/110", "eta": "22 days, 8:34:13", "gpu_mem": "2.46G", "iter": "1030/234583", "loss": 6.00141, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:15][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07533, "dt_data": 0.00180, "dt_net": 0.07353, "epoch": "1/110", "eta": "22 days, 11:56:00", "gpu_mem": "2.46G", "iter": "1040/234583", "loss": 5.99259, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:16][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07806, "dt_data": 0.00211, "dt_net": 0.07595, "epoch": "1/110", "eta": "23 days, 7:31:40", "gpu_mem": "2.46G", "iter": "1050/234583", "loss": 5.99408, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:17][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07766, "dt_data": 0.00203, "dt_net": 0.07563, "epoch": "1/110", "eta": "23 days, 4:38:44", "gpu_mem": "2.46G", "iter": "1060/234583", "loss": 5.99439, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:17][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07426, "dt_data": 0.00164, "dt_net": 0.07263, "epoch": "1/110", "eta": "22 days, 4:16:54", "gpu_mem": "2.46G", "iter": "1070/234583", "loss": 5.99068, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:18][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07757, "dt_data": 0.00217, "dt_net": 0.07540, "epoch": "1/110", "eta": "23 days, 4:00:10", "gpu_mem": "2.46G", "iter": "1080/234583", "loss": 5.99641, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:19][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07525, "dt_data": 0.00185, "dt_net": 0.07340, "epoch": "1/110", "eta": "22 days, 11:19:36", "gpu_mem": "2.46G", "iter": "1090/234583", "loss": 5.99486, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:20][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07486, "dt_data": 0.00191, "dt_net": 0.07295, "epoch": "1/110", "eta": "22 days, 8:32:35", "gpu_mem": "2.46G", "iter": "1100/234583", "loss": 5.99229, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:20][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07657, "dt_data": 0.00186, "dt_net": 0.07471, "epoch": "1/110", "eta": "22 days, 20:48:31", "gpu_mem": "2.46G", "iter": "1110/234583", "loss": 5.99637, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:21][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07504, "dt_data": 0.00176, "dt_net": 0.07329, "epoch": "1/110", "eta": "22 days, 9:52:57", "gpu_mem": "2.46G", "iter": "1120/234583", "loss": 5.99163, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:22][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07620, "dt_data": 0.00192, "dt_net": 0.07428, "epoch": "1/110", "eta": "22 days, 18:09:41", "gpu_mem": "2.46G", "iter": "1130/234583", "loss": 5.99469, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:23][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07597, "dt_data": 0.00252, "dt_net": 0.07344, "epoch": "1/110", "eta": "22 days, 16:28:59", "gpu_mem": "2.46G", "iter": "1140/234583", "loss": 5.99373, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:24][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07617, "dt_data": 0.00189, "dt_net": 0.07428, "epoch": "1/110", "eta": "22 days, 17:57:53", "gpu_mem": "2.46G", "iter": "1150/234583", "loss": 5.99578, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:19:24][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.07719, "dt_data": 0.00214, "dt_net": 0.07504, "epoch": "1/110", "eta": "23 days, 1:14:13", "gpu_mem": "2.46G", "iter": "1160/234583", "loss": 5.98905, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:21:02][INFO] train_net.py: 408: Train with config:
[06/12 15:21:02][INFO] train_net.py: 409: CfgNode({'BN': CfgNode({'USE_PRECISE_STATS': False, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1}), 'TRAIN': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 8, 'EVAL_PERIOD': 5, 'CHECKPOINT_PERIOD': 1, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': ()}), 'AUG': CfgNode({'ENABLE': True, 'NUM_SAMPLE': 2, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False}), 'MIXUP': CfgNode({'ENABLE': True, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 64, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 1, 'NUM_SPATIAL_CROPS': 1, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'TEST_BEST': False}), 'RESNET': CfgNode({'TRANS_FUNC': 'bottleneck_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 1.0, 'DEPTH_FACTOR': 1.0, 'BOTTLENECK_FACTOR': 1.0, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'uniformer', 'MODEL_NAME': 'Uniformer', 'NUM_CLASSES': 400, 'LOSS_FUNC': 'soft_cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'uniformer'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'USE_CHECKPOINT': False, 'CHECKPOINT_NUM': [0, 0, 0, 0]}), 'MVIT': CfgNode({'MODE': 'conv', 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'UNIFORMER': CfgNode({'EMBED_DIM': [64, 128, 320, 512], 'DEPTH': [3, 4, 8, 3], 'HEAD_DIM': 64, 'MLP_RATIO': 4, 'QKV_BIAS': True, 'QKV_SCALE': None, 'REPRESENTATION_SIZE': None, 'DROPOUT_RATE': 0, 'ATTENTION_DROPOUT_RATE': 0, 'DROP_DEPTH_RATE': 0.1, 'PRETRAIN_NAME': 'uniformer_small_in1k', 'SPLIT': False, 'STAGE_TYPE': [0, 0, 1, 1], 'STD': False, 'PRUNE_RATIO': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], 'TRADE_OFF': [[], [], [1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': './data_list/k400', 'PATH_LABEL_SEPARATOR': ',', 'PATH_PREFIX': '/super_faster_home/dataset/kinetics400', 'LABEL_PATH_TEMPLATE': 'somesomev1_rgb_{}_split.txt', 'IMAGE_TEMPLATE': '{:05d}.jpg', 'NUM_FRAMES': 16, 'SAMPLING_RATE': 4, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [256, 320], 'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0], 'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333], 'USE_OFFSET_SAMPLING': True, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 224, 'TEST_CROP_SIZE': 224, 'TARGET_FPS': 30, 'DECODING_BACKEND': 'decord', 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False}), 'SOLVER': CfgNode({'BASE_LR': 0.0001, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 1e-06, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 110, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 0.05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 10.0, 'WARMUP_START_LR': 1e-06, 'OPTIMIZING_METHOD': 'adamw', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': True, 'ZERO_WD_1D_PARAM': True, 'CLIP_GRADIENT': 20}), 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': './exp/uniformer_s16x4_k400', 'RNG_SEED': 6666, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': True, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[06/12 15:21:02][INFO] uniformer.py: 288: Use checkpoint: False
[06/12 15:21:02][INFO] uniformer.py: 289: Checkpoint number: [0, 0, 0, 0]
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: patch_embed1.proj.weight, torch.Size([64, 3, 4, 4]) => torch.Size([64, 3, 3, 4, 4])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: patch_embed2.proj.weight, torch.Size([128, 64, 2, 2]) => torch.Size([128, 64, 1, 2, 2])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: patch_embed3.proj.weight, torch.Size([320, 128, 2, 2]) => torch.Size([320, 128, 1, 2, 2])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: patch_embed4.proj.weight, torch.Size([512, 320, 2, 2]) => torch.Size([512, 320, 1, 2, 2])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.0.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.0.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.0.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.0.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.0.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.1.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.1.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.1.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.1.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.1.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.2.pos_embed.weight, torch.Size([64, 1, 3, 3]) => torch.Size([64, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.2.conv1.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.2.conv2.weight, torch.Size([64, 64, 1, 1]) => torch.Size([64, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.2.attn.weight, torch.Size([64, 1, 5, 5]) => torch.Size([64, 1, 5, 5, 5])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc1.weight, torch.Size([256, 64, 1, 1]) => torch.Size([256, 64, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks1.2.mlp.fc2.weight, torch.Size([64, 256, 1, 1]) => torch.Size([64, 256, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.0.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.0.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.0.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.0.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.0.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.1.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.1.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.1.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.1.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.1.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.2.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.2.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.2.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.2.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.2.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.3.pos_embed.weight, torch.Size([128, 1, 3, 3]) => torch.Size([128, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.3.conv1.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.3.conv2.weight, torch.Size([128, 128, 1, 1]) => torch.Size([128, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.3.attn.weight, torch.Size([128, 1, 5, 5]) => torch.Size([128, 1, 5, 5, 5])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc1.weight, torch.Size([512, 128, 1, 1]) => torch.Size([512, 128, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks2.3.mlp.fc2.weight, torch.Size([128, 512, 1, 1]) => torch.Size([128, 512, 1, 1, 1])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.0.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.1.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.2.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.3.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.4.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.5.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.6.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks3.7.pos_embed.weight, torch.Size([320, 1, 3, 3]) => torch.Size([320, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks4.0.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks4.1.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 413: Inflate: blocks4.2.pos_embed.weight, torch.Size([512, 1, 3, 3]) => torch.Size([512, 1, 3, 3, 3])
[06/12 15:21:03][INFO] uniformer.py: 411: Ignore: head.weight
[06/12 15:21:03][INFO] uniformer.py: 411: Ignore: head.bias
[06/12 15:21:03][INFO] build.py:  45: load pretrained model
[06/12 15:21:03][INFO] misc.py: 183: Model:
Uniformer(
  (patch_embed1): SpeicalPatchEmbed(
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(3, 64, kernel_size=(3, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0))
  )
  (patch_embed2): PatchEmbed(
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed3): PatchEmbed(
    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(128, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (patch_embed4): PatchEmbed(
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Conv3d(320, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  )
  (pos_drop): Dropout(p=0, inplace=False)
  (blocks1): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): Identity()
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.006)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64)
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=64)
      (drop_path): DropPath(drop_prob=0.012)
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.018)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.024)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.029)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): CBlock(
      (pos_embed): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128)
      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (conv2): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (attn): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=128)
      (drop_path): DropPath(drop_prob=0.035)
      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (mlp): CMlp(
        (fc1): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): GELU(approximate='none')
        (fc2): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.041)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.047)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.053)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.059)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.065)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.071)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (6): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.076)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (7): SABlock(
      (pos_embed): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=320)
      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=320, out_features=960, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=320, out_features=320, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=320, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=320, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (blocks4): ModuleList(
    (0): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.088)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.094)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): SABlock(
      (pos_embed): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pre_logits): Identity()
  (head): Linear(in_features=512, out_features=400, bias=True)
)
[06/12 15:21:03][INFO] misc.py: 184: Params: 21,400,400
[06/12 15:21:03][INFO] misc.py: 185: Mem: 0.0800790786743164 MB
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:21:03][INFO] misc.py: 186: Flops: 41.756772352 G
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::layer_norm encountered 26 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::add encountered 54 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::batch_norm encountered 15 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::gelu encountered 18 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::mul encountered 11 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::softmax encountered 11 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 499: Unsupported operator aten::mean encountered 1 time(s)
[06/12 15:21:03][WARNING] jit_analysis.py: 511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks1.1.drop_path, blocks1.2.drop_path, blocks2.0.drop_path, blocks2.1.drop_path, blocks2.2.drop_path, blocks2.3.drop_path, blocks3.0.drop_path, blocks3.1.drop_path, blocks3.2.drop_path, blocks3.3.drop_path, blocks3.4.drop_path, blocks3.5.drop_path, blocks3.6.drop_path, blocks3.7.drop_path, blocks4.0.drop_path, blocks4.1.drop_path, blocks4.2.drop_path
[06/12 15:21:03][INFO] misc.py: 191: Activations: 228.17576 M
[06/12 15:21:03][INFO] misc.py: 196: nvidia-smi
[06/12 15:21:03][INFO] kinetics.py:  76: Constructing Kinetics train...
[06/12 15:21:04][INFO] kinetics.py: 123: Constructing kinetics dataloader (size: 234583) from ./data_list/k400/train.csv
[06/12 15:21:04][INFO] kinetics.py:  76: Constructing Kinetics val...
[06/12 15:21:04][INFO] kinetics.py: 123: Constructing kinetics dataloader (size: 19760) from ./data_list/k400/val.csv
[06/12 15:21:04][INFO] tensorboard_vis.py:  54: To see logged results in Tensorboard, please launch using the command             `tensorboard  --port=<port-number> --logdir ./exp/uniformer_s16x4_k400/runs-kinetics`
[06/12 15:21:04][INFO] train_net.py: 451: Start epoch: 1
[06/12 15:21:30][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.50273, "dt_data": 0.01920, "dt_net": 0.48353, "epoch": "1/110", "eta": "18 days, 18:24:58", "gpu_mem": "17.25G", "iter": "10/29322", "loss": 5.99628, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:21:34][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48391, "dt_data": 0.01750, "dt_net": 0.46641, "epoch": "1/110", "eta": "18 days, 1:33:25", "gpu_mem": "17.25G", "iter": "20/29322", "loss": 6.02831, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:21:39][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47812, "dt_data": 0.01319, "dt_net": 0.46492, "epoch": "1/110", "eta": "17 days, 20:21:50", "gpu_mem": "17.25G", "iter": "30/29322", "loss": 6.01881, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:21:44][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48593, "dt_data": 0.01565, "dt_net": 0.47028, "epoch": "1/110", "eta": "18 days, 3:21:46", "gpu_mem": "17.25G", "iter": "40/29322", "loss": 6.01922, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:21:49][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48555, "dt_data": 0.01699, "dt_net": 0.46857, "epoch": "1/110", "eta": "18 days, 3:01:31", "gpu_mem": "17.25G", "iter": "50/29322", "loss": 6.00867, "lr": 0.00000, "top1_err": 93.75000, "top5_err": 93.75000}
[06/12 15:21:54][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48846, "dt_data": 0.01997, "dt_net": 0.46849, "epoch": "1/110", "eta": "18 days, 5:37:29", "gpu_mem": "17.25G", "iter": "60/29322", "loss": 6.00546, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:21:59][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48493, "dt_data": 0.01474, "dt_net": 0.47020, "epoch": "1/110", "eta": "18 days, 2:27:59", "gpu_mem": "17.25G", "iter": "70/29322", "loss": 5.97856, "lr": 0.00000, "top1_err": 93.75000, "top5_err": 93.75000}
[06/12 15:22:04][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48773, "dt_data": 0.01501, "dt_net": 0.47271, "epoch": "1/110", "eta": "18 days, 4:58:01", "gpu_mem": "17.25G", "iter": "80/29322", "loss": 6.01440, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:22:08][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48365, "dt_data": 0.01400, "dt_net": 0.46965, "epoch": "1/110", "eta": "18 days, 1:18:47", "gpu_mem": "17.25G", "iter": "90/29322", "loss": 5.99790, "lr": 0.00000, "top1_err": 93.75000, "top5_err": 93.75000}
[06/12 15:22:13][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48021, "dt_data": 0.01422, "dt_net": 0.46599, "epoch": "1/110", "eta": "17 days, 22:13:36", "gpu_mem": "17.25G", "iter": "100/29322", "loss": 6.02271, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:22:18][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48229, "dt_data": 0.01583, "dt_net": 0.46646, "epoch": "1/110", "eta": "18 days, 0:05:41", "gpu_mem": "17.25G", "iter": "110/29322", "loss": 5.99911, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:22:23][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48657, "dt_data": 0.01680, "dt_net": 0.46978, "epoch": "1/110", "eta": "18 days, 3:55:47", "gpu_mem": "17.25G", "iter": "120/29322", "loss": 6.02296, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 93.75000}
[06/12 15:22:28][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47941, "dt_data": 0.01275, "dt_net": 0.46666, "epoch": "1/110", "eta": "17 days, 21:30:44", "gpu_mem": "17.25G", "iter": "130/29322", "loss": 6.01969, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:22:33][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47910, "dt_data": 0.01302, "dt_net": 0.46608, "epoch": "1/110", "eta": "17 days, 21:14:02", "gpu_mem": "17.25G", "iter": "140/29322", "loss": 6.02804, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:22:37][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48068, "dt_data": 0.01439, "dt_net": 0.46629, "epoch": "1/110", "eta": "17 days, 22:38:44", "gpu_mem": "17.25G", "iter": "150/29322", "loss": 6.00498, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:22:42][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48349, "dt_data": 0.01756, "dt_net": 0.46593, "epoch": "1/110", "eta": "18 days, 1:09:33", "gpu_mem": "17.25G", "iter": "160/29322", "loss": 5.99410, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:22:47][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48264, "dt_data": 0.01408, "dt_net": 0.46856, "epoch": "1/110", "eta": "18 days, 0:23:55", "gpu_mem": "17.25G", "iter": "170/29322", "loss": 6.03890, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:22:52][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48005, "dt_data": 0.01357, "dt_net": 0.46647, "epoch": "1/110", "eta": "17 days, 22:04:27", "gpu_mem": "17.25G", "iter": "180/29322", "loss": 6.03292, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:22:57][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48386, "dt_data": 0.01808, "dt_net": 0.46578, "epoch": "1/110", "eta": "18 days, 1:29:20", "gpu_mem": "17.25G", "iter": "190/29322", "loss": 5.96306, "lr": 0.00000, "top1_err": 93.75000, "top5_err": 90.62500}
[06/12 15:23:02][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48469, "dt_data": 0.01864, "dt_net": 0.46604, "epoch": "1/110", "eta": "18 days, 2:13:47", "gpu_mem": "17.25G", "iter": "200/29322", "loss": 6.00911, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 93.75000}
[06/12 15:23:06][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47850, "dt_data": 0.01397, "dt_net": 0.46452, "epoch": "1/110", "eta": "17 days, 20:40:48", "gpu_mem": "17.25G", "iter": "210/29322", "loss": 6.03238, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:23:11][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48031, "dt_data": 0.01430, "dt_net": 0.46602, "epoch": "1/110", "eta": "17 days, 22:18:28", "gpu_mem": "17.25G", "iter": "220/29322", "loss": 6.03229, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 90.62500}
[06/12 15:23:16][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48172, "dt_data": 0.01363, "dt_net": 0.46809, "epoch": "1/110", "eta": "17 days, 23:33:58", "gpu_mem": "17.25G", "iter": "230/29322", "loss": 5.99780, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 93.75000}
[06/12 15:23:21][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48189, "dt_data": 0.01383, "dt_net": 0.46806, "epoch": "1/110", "eta": "17 days, 23:42:53", "gpu_mem": "17.25G", "iter": "240/29322", "loss": 6.00255, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:23:26][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48492, "dt_data": 0.01612, "dt_net": 0.46880, "epoch": "1/110", "eta": "18 days, 2:25:46", "gpu_mem": "17.25G", "iter": "250/29322", "loss": 6.01434, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:23:30][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47927, "dt_data": 0.01345, "dt_net": 0.46581, "epoch": "1/110", "eta": "17 days, 21:22:00", "gpu_mem": "17.25G", "iter": "260/29322", "loss": 6.02233, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:23:35][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48192, "dt_data": 0.01476, "dt_net": 0.46716, "epoch": "1/110", "eta": "17 days, 23:44:23", "gpu_mem": "17.25G", "iter": "270/29322", "loss": 6.01991, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:23:40][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47773, "dt_data": 0.01270, "dt_net": 0.46503, "epoch": "1/110", "eta": "17 days, 19:59:05", "gpu_mem": "17.25G", "iter": "280/29322", "loss": 6.00637, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:23:45][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48332, "dt_data": 0.01459, "dt_net": 0.46873, "epoch": "1/110", "eta": "18 days, 0:59:35", "gpu_mem": "17.25G", "iter": "290/29322", "loss": 6.01508, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:23:50][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47915, "dt_data": 0.01302, "dt_net": 0.46613, "epoch": "1/110", "eta": "17 days, 21:15:17", "gpu_mem": "17.25G", "iter": "300/29322", "loss": 6.03085, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:23:55][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48018, "dt_data": 0.01384, "dt_net": 0.46635, "epoch": "1/110", "eta": "17 days, 22:10:46", "gpu_mem": "17.25G", "iter": "310/29322", "loss": 6.01398, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:23:59][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48097, "dt_data": 0.01328, "dt_net": 0.46768, "epoch": "1/110", "eta": "17 days, 22:52:46", "gpu_mem": "17.25G", "iter": "320/29322", "loss": 6.02906, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:24:04][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48799, "dt_data": 0.02152, "dt_net": 0.46647, "epoch": "1/110", "eta": "18 days, 5:10:11", "gpu_mem": "17.25G", "iter": "330/29322", "loss": 5.99809, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:24:09][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48922, "dt_data": 0.01369, "dt_net": 0.47553, "epoch": "1/110", "eta": "18 days, 6:16:09", "gpu_mem": "17.25G", "iter": "340/29322", "loss": 5.99715, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:24:14][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47901, "dt_data": 0.01236, "dt_net": 0.46664, "epoch": "1/110", "eta": "17 days, 21:07:14", "gpu_mem": "17.25G", "iter": "350/29322", "loss": 5.98710, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:24:19][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48395, "dt_data": 0.01567, "dt_net": 0.46828, "epoch": "1/110", "eta": "18 days, 1:33:03", "gpu_mem": "17.25G", "iter": "360/29322", "loss": 6.00539, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:24:24][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48038, "dt_data": 0.01318, "dt_net": 0.46719, "epoch": "1/110", "eta": "17 days, 22:20:36", "gpu_mem": "17.25G", "iter": "370/29322", "loss": 6.02458, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:24:28][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48839, "dt_data": 0.02218, "dt_net": 0.46621, "epoch": "1/110", "eta": "18 days, 5:31:16", "gpu_mem": "17.25G", "iter": "380/29322", "loss": 6.03207, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:24:33][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48300, "dt_data": 0.01582, "dt_net": 0.46718, "epoch": "1/110", "eta": "18 days, 0:41:16", "gpu_mem": "17.25G", "iter": "390/29322", "loss": 6.01374, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:24:38][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47912, "dt_data": 0.01394, "dt_net": 0.46518, "epoch": "1/110", "eta": "17 days, 21:12:40", "gpu_mem": "17.25G", "iter": "400/29322", "loss": 6.05159, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:24:43][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48030, "dt_data": 0.01401, "dt_net": 0.46629, "epoch": "1/110", "eta": "17 days, 22:16:13", "gpu_mem": "17.25G", "iter": "410/29322", "loss": 5.99044, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:24:48][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48455, "dt_data": 0.01825, "dt_net": 0.46630, "epoch": "1/110", "eta": "18 days, 2:04:48", "gpu_mem": "17.25G", "iter": "420/29322", "loss": 6.02045, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:24:53][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48246, "dt_data": 0.01753, "dt_net": 0.46493, "epoch": "1/110", "eta": "18 days, 0:12:03", "gpu_mem": "17.25G", "iter": "430/29322", "loss": 6.02989, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:24:57][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48165, "dt_data": 0.01296, "dt_net": 0.46869, "epoch": "1/110", "eta": "17 days, 23:28:24", "gpu_mem": "17.25G", "iter": "440/29322", "loss": 6.03559, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:25:02][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47748, "dt_data": 0.01278, "dt_net": 0.46470, "epoch": "1/110", "eta": "17 days, 19:44:03", "gpu_mem": "17.25G", "iter": "450/29322", "loss": 5.99059, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:25:07][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48147, "dt_data": 0.01285, "dt_net": 0.46862, "epoch": "1/110", "eta": "17 days, 23:18:31", "gpu_mem": "17.25G", "iter": "460/29322", "loss": 6.01522, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 90.62500}
[06/12 15:25:12][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48607, "dt_data": 0.01995, "dt_net": 0.46612, "epoch": "1/110", "eta": "18 days, 3:25:45", "gpu_mem": "17.25G", "iter": "470/29322", "loss": 6.01952, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:25:17][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48530, "dt_data": 0.02008, "dt_net": 0.46522, "epoch": "1/110", "eta": "18 days, 2:44:33", "gpu_mem": "17.25G", "iter": "480/29322", "loss": 6.01228, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:25:21][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47993, "dt_data": 0.01432, "dt_net": 0.46561, "epoch": "1/110", "eta": "17 days, 21:55:45", "gpu_mem": "17.25G", "iter": "490/29322", "loss": 6.01038, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:25:26][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48320, "dt_data": 0.01759, "dt_net": 0.46561, "epoch": "1/110", "eta": "18 days, 0:51:36", "gpu_mem": "17.25G", "iter": "500/29322", "loss": 6.03307, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:25:31][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48192, "dt_data": 0.01449, "dt_net": 0.46743, "epoch": "1/110", "eta": "17 days, 23:42:26", "gpu_mem": "17.25G", "iter": "510/29322", "loss": 5.96320, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 87.50000}
[06/12 15:25:36][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48126, "dt_data": 0.01339, "dt_net": 0.46787, "epoch": "1/110", "eta": "17 days, 23:06:45", "gpu_mem": "17.25G", "iter": "520/29322", "loss": 5.97620, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:25:41][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48511, "dt_data": 0.01822, "dt_net": 0.46690, "epoch": "1/110", "eta": "18 days, 2:33:58", "gpu_mem": "17.25G", "iter": "530/29322", "loss": 5.99588, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:25:46][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48649, "dt_data": 0.01911, "dt_net": 0.46737, "epoch": "1/110", "eta": "18 days, 3:47:49", "gpu_mem": "17.25G", "iter": "540/29322", "loss": 6.00984, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:25:50][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48780, "dt_data": 0.02060, "dt_net": 0.46721, "epoch": "1/110", "eta": "18 days, 4:58:26", "gpu_mem": "17.25G", "iter": "550/29322", "loss": 6.01028, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:25:55][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47896, "dt_data": 0.01329, "dt_net": 0.46567, "epoch": "1/110", "eta": "17 days, 21:03:02", "gpu_mem": "17.25G", "iter": "560/29322", "loss": 6.01333, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:26:00][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48199, "dt_data": 0.01460, "dt_net": 0.46739, "epoch": "1/110", "eta": "17 days, 23:45:51", "gpu_mem": "17.25G", "iter": "570/29322", "loss": 6.02133, "lr": 0.00000, "top1_err": 93.75000, "top5_err": 93.75000}
[06/12 15:26:05][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48464, "dt_data": 0.01882, "dt_net": 0.46582, "epoch": "1/110", "eta": "18 days, 2:08:22", "gpu_mem": "17.25G", "iter": "580/29322", "loss": 6.03545, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:26:10][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48341, "dt_data": 0.01621, "dt_net": 0.46719, "epoch": "1/110", "eta": "18 days, 1:01:51", "gpu_mem": "17.25G", "iter": "590/29322", "loss": 5.99194, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:26:15][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48383, "dt_data": 0.01709, "dt_net": 0.46675, "epoch": "1/110", "eta": "18 days, 1:24:39", "gpu_mem": "17.25G", "iter": "600/29322", "loss": 5.99794, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 93.75000}
[06/12 15:26:19][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47781, "dt_data": 0.01324, "dt_net": 0.46457, "epoch": "1/110", "eta": "17 days, 20:00:44", "gpu_mem": "17.25G", "iter": "610/29322", "loss": 6.02776, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:26:24][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48429, "dt_data": 0.01453, "dt_net": 0.46976, "epoch": "1/110", "eta": "18 days, 1:49:01", "gpu_mem": "17.25G", "iter": "620/29322", "loss": 5.99255, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 87.50000}
[06/12 15:26:29][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48215, "dt_data": 0.01638, "dt_net": 0.46578, "epoch": "1/110", "eta": "17 days, 23:54:08", "gpu_mem": "17.25G", "iter": "630/29322", "loss": 6.03390, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:26:34][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48583, "dt_data": 0.01773, "dt_net": 0.46810, "epoch": "1/110", "eta": "18 days, 3:11:48", "gpu_mem": "17.25G", "iter": "640/29322", "loss": 5.98091, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 93.75000}
[06/12 15:26:39][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48333, "dt_data": 0.01661, "dt_net": 0.46673, "epoch": "1/110", "eta": "18 days, 0:57:22", "gpu_mem": "17.25G", "iter": "650/29322", "loss": 6.03413, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:26:43][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48440, "dt_data": 0.01849, "dt_net": 0.46592, "epoch": "1/110", "eta": "18 days, 1:54:49", "gpu_mem": "17.25G", "iter": "660/29322", "loss": 6.00015, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:26:48][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48058, "dt_data": 0.01506, "dt_net": 0.46552, "epoch": "1/110", "eta": "17 days, 22:29:03", "gpu_mem": "17.25G", "iter": "670/29322", "loss": 6.01620, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:26:53][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48135, "dt_data": 0.01582, "dt_net": 0.46554, "epoch": "1/110", "eta": "17 days, 23:10:44", "gpu_mem": "17.25G", "iter": "680/29322", "loss": 5.97986, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:26:58][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48166, "dt_data": 0.01614, "dt_net": 0.46552, "epoch": "1/110", "eta": "17 days, 23:27:11", "gpu_mem": "17.25G", "iter": "690/29322", "loss": 6.01321, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:27:03][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48307, "dt_data": 0.01348, "dt_net": 0.46959, "epoch": "1/110", "eta": "18 days, 0:42:39", "gpu_mem": "17.25G", "iter": "700/29322", "loss": 6.04223, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:27:08][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48268, "dt_data": 0.01697, "dt_net": 0.46571, "epoch": "1/110", "eta": "18 days, 0:21:41", "gpu_mem": "17.25G", "iter": "710/29322", "loss": 6.01374, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:27:12][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48048, "dt_data": 0.01363, "dt_net": 0.46685, "epoch": "1/110", "eta": "17 days, 22:23:07", "gpu_mem": "17.25G", "iter": "720/29322", "loss": 6.02603, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:27:17][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47944, "dt_data": 0.01284, "dt_net": 0.46661, "epoch": "1/110", "eta": "17 days, 21:27:33", "gpu_mem": "17.25G", "iter": "730/29322", "loss": 5.99033, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:27:22][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48137, "dt_data": 0.01586, "dt_net": 0.46551, "epoch": "1/110", "eta": "17 days, 23:11:11", "gpu_mem": "17.25G", "iter": "740/29322", "loss": 6.01512, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:27:27][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48709, "dt_data": 0.01954, "dt_net": 0.46754, "epoch": "1/110", "eta": "18 days, 4:18:11", "gpu_mem": "17.25G", "iter": "750/29322", "loss": 5.99742, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:27:32][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47905, "dt_data": 0.01235, "dt_net": 0.46670, "epoch": "1/110", "eta": "17 days, 21:06:27", "gpu_mem": "17.25G", "iter": "760/29322", "loss": 5.98758, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:27:37][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48234, "dt_data": 0.01464, "dt_net": 0.46770, "epoch": "1/110", "eta": "18 days, 0:02:51", "gpu_mem": "17.25G", "iter": "770/29322", "loss": 6.02379, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:27:41][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48802, "dt_data": 0.01952, "dt_net": 0.46850, "epoch": "1/110", "eta": "18 days, 5:08:07", "gpu_mem": "17.25G", "iter": "780/29322", "loss": 6.02315, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:27:46][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48145, "dt_data": 0.01382, "dt_net": 0.46763, "epoch": "1/110", "eta": "17 days, 23:15:09", "gpu_mem": "17.25G", "iter": "790/29322", "loss": 6.01047, "lr": 0.00000, "top1_err": 93.75000, "top5_err": 93.75000}
[06/12 15:27:51][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48703, "dt_data": 0.01929, "dt_net": 0.46774, "epoch": "1/110", "eta": "18 days, 4:14:50", "gpu_mem": "17.25G", "iter": "800/29322", "loss": 6.02703, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 100.00000}
[06/12 15:27:56][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48354, "dt_data": 0.01304, "dt_net": 0.47050, "epoch": "1/110", "eta": "18 days, 1:06:56", "gpu_mem": "17.25G", "iter": "810/29322", "loss": 5.99491, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:28:01][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48259, "dt_data": 0.01633, "dt_net": 0.46626, "epoch": "1/110", "eta": "18 days, 0:15:49", "gpu_mem": "17.25G", "iter": "820/29322", "loss": 5.98227, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 93.75000}
[06/12 15:28:06][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48114, "dt_data": 0.01386, "dt_net": 0.46728, "epoch": "1/110", "eta": "17 days, 22:58:08", "gpu_mem": "17.25G", "iter": "830/29322", "loss": 5.98021, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 96.87500}
[06/12 15:28:10][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47951, "dt_data": 0.01263, "dt_net": 0.46688, "epoch": "1/110", "eta": "17 days, 21:30:27", "gpu_mem": "17.25G", "iter": "840/29322", "loss": 6.00261, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:28:15][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48109, "dt_data": 0.01421, "dt_net": 0.46688, "epoch": "1/110", "eta": "17 days, 22:55:15", "gpu_mem": "17.25G", "iter": "850/29322", "loss": 5.97519, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:28:20][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47819, "dt_data": 0.01270, "dt_net": 0.46549, "epoch": "1/110", "eta": "17 days, 20:19:13", "gpu_mem": "17.25G", "iter": "860/29322", "loss": 5.98974, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:28:25][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47761, "dt_data": 0.01257, "dt_net": 0.46504, "epoch": "1/110", "eta": "17 days, 19:47:42", "gpu_mem": "17.25G", "iter": "870/29322", "loss": 6.00184, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:28:30][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48091, "dt_data": 0.01331, "dt_net": 0.46761, "epoch": "1/110", "eta": "17 days, 22:45:17", "gpu_mem": "17.25G", "iter": "880/29322", "loss": 6.01072, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 93.75000}
[06/12 15:28:34][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.48196, "dt_data": 0.01421, "dt_net": 0.46775, "epoch": "1/110", "eta": "17 days, 23:41:24", "gpu_mem": "17.25G", "iter": "890/29322", "loss": 6.00929, "lr": 0.00000, "top1_err": 96.87500, "top5_err": 93.75000}
[06/12 15:28:39][INFO] logging.py:  99: json_stats: {"_type": "train_iter", "dt": 0.47868, "dt_data": 0.01257, "dt_net": 0.46611, "epoch": "1/110", "eta": "17 days, 20:45:14", "gpu_mem": "17.25G", "iter": "900/29322", "loss": 6.01068, "lr": 0.00000, "top1_err": 100.00000, "top5_err": 90.62500}
